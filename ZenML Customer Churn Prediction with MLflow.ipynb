{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dff0960-95f2-4e43-9c01-7409fbb12c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read some customer data from the Aqueduct repo.\n",
    "customers_table = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/aqueducthq/aqueduct/main/examples/churn_prediction/data/customers.csv\"\n",
    ")\n",
    "churn_table = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/aqueducthq/aqueduct/main/examples/churn_prediction/data/churn_data.csv\"\n",
    ")\n",
    "pd.merge(customers_table, churn_table, on=\"cust_id\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3f50bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.steps import step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8960448-0658-4bfc-aac6-f87fa9b415af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The @op decorator here allows Aqueduct to run this function as\n",
    "# a part of an Aqueduct workflow. It tells Aqueduct that when\n",
    "# we execute this function, we're defining a step in the workflow.\n",
    "# While the results can be retrieved immediately, nothing is\n",
    "# published until we call `publish_flow()` below.\n",
    "@step\n",
    "def log_featurize(cust: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    log_featurize takes in customer data from the Aqueduct customers table\n",
    "    and log normalizes the numerical columns using the numpy.log function.\n",
    "    It skips the cust_id, using_deep_learning, and using_dbt columns because\n",
    "    these are not numerical columns that require regularization.\n",
    "\n",
    "    log_featurize adds all the log-normalized values into new columns, and\n",
    "    maintains the original values as-is. In addition to the original company_size\n",
    "    column, log_featurize will add a log_company_size column.\n",
    "    \"\"\"\n",
    "    features = cust.copy()\n",
    "    skip_cols = [\"cust_id\", \"using_deep_learning\", \"using_dbt\"]\n",
    "\n",
    "    for col in features.columns.difference(skip_cols):\n",
    "        features[\"log_\" + col] = np.log(features[col] + 1.0)\n",
    "\n",
    "    return features.drop(columns=\"cust_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6f4d6b-7795-4043-b724-d2f9fe5483b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling `.local()` on an @op-annotated function allows us to execute the\n",
    "# function locally for testing purposes. When a function is called with\n",
    "# `.local()`, Aqueduct does not capture the function execution as a part of\n",
    "# the definition of a workflow.\n",
    "features_table = log_featurize.entrypoint(customers_table)\n",
    "features_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac199805-fe08-496e-b282-84710439df39",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "In this example, we will train and ensemble two basic classifiers.  In practice, would probably do something more interesting but this will help illustrate post-processing logic (the ensemble function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32d9e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be14fd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def data_loader() -> pd.DataFrame:\n",
    "    # Read some customer data from the Aqueduct repo.\n",
    "    customers_table = pd.read_csv(\n",
    "        \"https://raw.githubusercontent.com/aqueducthq/aqueduct/main/examples/churn_prediction/data/customers.csv\"\n",
    "    )\n",
    "    \n",
    "    return customers_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da26b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def label_loader() -> pd.DataFrame:\n",
    "    # Read some customer data from the Aqueduct repo.\n",
    "    churn_table = pd.read_csv(\n",
    "        \"https://raw.githubusercontent.com/aqueducthq/aqueduct/main/examples/churn_prediction/data/churn_data.csv\"\n",
    "    )\n",
    "    \n",
    "    return churn_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc2a19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from zenml.client import Client\n",
    "\n",
    "experiment_tracker = Client().active_stack.experiment_tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba782d5",
   "metadata": {},
   "source": [
    "Instead of training static models like in our previous notebook, let's include the training steps into the pipeline itself and deploy the trained models using ZenML's MLflow integration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c3b827",
   "metadata": {},
   "outputs": [],
   "source": [
    "@step(enable_cache=False, experiment_tracker=experiment_tracker.name)\n",
    "def train_linear_model(\n",
    "    feature_table: pd.DataFrame,\n",
    "    churn_table: pd.DataFrame\n",
    ") -> sklearn.linear_model.LogisticRegression:\n",
    "    mlflow.sklearn.autolog()  # log all model hparams and metrics to MLflow\n",
    "    linear_model = LogisticRegression(max_iter=10000)\n",
    "    linear_model.fit(features_table, churn_table[\"churn\"])\n",
    "    return linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a185333",
   "metadata": {},
   "outputs": [],
   "source": [
    "@step(enable_cache=False, experiment_tracker=experiment_tracker.name)\n",
    "def train_decision_tree_model(\n",
    "    feature_table: pd.DataFrame,\n",
    "    churn_table: pd.DataFrame\n",
    ") -> sklearn.tree.DecisionTreeClassifier:\n",
    "    mlflow.sklearn.autolog()  # log all model hparams and metrics to MLflow\n",
    "    decision_tree_model = DecisionTreeClassifier(max_depth=10, min_samples_split=3)\n",
    "    decision_tree_model.fit(features_table, churn_table[\"churn\"])\n",
    "    return decision_tree_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4436e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def deployment_trigger() -> bool:\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a1c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.pipelines import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2210a561",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(enable_cache=False)\n",
    "def churn_training_pipeline(\n",
    "    data_loader,\n",
    "    label_loader,\n",
    "    log_featurize,\n",
    "    train_linear,\n",
    "    train_tree,\n",
    "    deployment_trigger,\n",
    "    model_deployer_linear,\n",
    "    model_deployer_tree,\n",
    "):\n",
    "    customers_table = data_loader()\n",
    "    churn_table = label_loader()\n",
    "    features_table = log_featurize(customers_table)\n",
    "    linear_model = train_linear(features_table, churn_table)\n",
    "    tree_model = train_tree(features_table, churn_table)\n",
    "    trigger = deployment_trigger()\n",
    "    model_deployer_linear(trigger, linear_model)\n",
    "    model_deployer_tree(trigger, tree_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1386a1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.integrations.mlflow.steps import mlflow_model_deployer_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6fa0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deployer_linear_step = mlflow_model_deployer_step()\n",
    "model_deployer_linear_step.configure(name='mlflow_linear_model_deployer')\n",
    "model_deployer_tree_step = mlflow_model_deployer_step()\n",
    "model_deployer_tree_step.configure(name='mlflow_tree_model_deployer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf11e01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_training_pipeline(\n",
    "    data_loader=data_loader(),\n",
    "    label_loader=label_loader(),\n",
    "    log_featurize=log_featurize(),\n",
    "    train_linear=train_linear_model(),\n",
    "    train_tree=train_decision_tree_model(),\n",
    "    deployment_trigger=deployment_trigger(),\n",
    "    model_deployer_linear=model_deployer_linear_step,\n",
    "    model_deployer_tree=model_deployer_tree_step,\n",
    ").run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f17960",
   "metadata": {},
   "source": [
    "Now that we've built a training pipeline, let's build an inference pipeline that\n",
    "uses the linear model we deployed in the training pipeline above to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e04515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.services import BaseService\n",
    "\n",
    "@step(enable_cache=False)\n",
    "def linear_model_service_loader() -> BaseService:\n",
    "    client = Client()\n",
    "    model_deployer = client.active_stack.model_deployer\n",
    "    services = model_deployer.find_model_server(\n",
    "        pipeline_name=\"churn_training_pipeline\",\n",
    "        pipeline_step_name=\"mlflow_linear_model_deployer\",\n",
    "        running=True,\n",
    "    )\n",
    "    service = services[0]\n",
    "    return service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c79440c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.steps import Output\n",
    "\n",
    "@step\n",
    "def predictor(\n",
    "    service: BaseService,\n",
    "    features_table: pd.DataFrame,\n",
    ") -> Output(predictions=list):\n",
    "    \"\"\"Run a inference request against a prediction service\"\"\"\n",
    "    service.start(timeout=10)  # should be a NOP if already started\n",
    "    print(f\"Input is: {[features_table.to_numpy()]}\")\n",
    "    prediction = service.predict(features_table.to_numpy())\n",
    "    print(f\"Prediction is: {[prediction.tolist()]}\")\n",
    "    return [prediction.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5742d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(enable_cache=False)\n",
    "def churn_inference_pipeline(\n",
    "    data_loader,\n",
    "    log_featurize,\n",
    "    linear_model_service_loader,\n",
    "    predictor,\n",
    "):\n",
    "    customers_table = data_loader()\n",
    "    features_table = log_featurize(customers_table)\n",
    "    service = linear_model_service_loader()\n",
    "    predictor(service, features_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f13c003",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_inference_pipeline(\n",
    "    data_loader=data_loader(),\n",
    "    log_featurize=log_featurize(),\n",
    "    linear_model_service_loader=linear_model_service_loader(),\n",
    "    predictor=predictor(),\n",
    ").run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95b7505",
   "metadata": {},
   "source": [
    "Lastly, remember we added MLflow experiment tracking (`mlflow.sklearn.autolog()`) to our training steps before? Those two simple lines of code automatically configured and initialized MLflow and logged all hyperparameters and metrics there.\n",
    "\n",
    "Let's start up the MLflow UI and check it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82af1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.environment import Environment\n",
    "from zenml.integrations.mlflow.mlflow_utils import get_tracking_uri\n",
    "\n",
    "\n",
    "def open_mlflow_ui(port=4997):\n",
    "    if Environment.in_google_colab():\n",
    "        from pyngrok import ngrok\n",
    "\n",
    "        public_url = ngrok.connect(port)\n",
    "        print(f\"\\x1b[31mIn Colab, use this URL instead: {public_url}!\\x1b[0m\")\n",
    "\n",
    "    !mlflow ui --backend-store-uri=\"{get_tracking_uri()}\" --port={port}\n",
    "\n",
    "\n",
    "open_mlflow_ui()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
